{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["NWfB8kThhHXI"],"mount_file_id":"1wTeojqqBn6NFkse0TcFIfRc8nQ8D2tYF","authorship_tag":"ABX9TyPAH91CG6rR9gqt/F/HbjZz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Punctuation Errorifier-Tagger for the new HuggingFace training pipeline\n","\n","Written by Artem Yushko\n","\n","Last edit made by Artem: 25/07/2022  *(edit this line if you change something)*"],"metadata":{"id":"oKXk-pv77fgq"}},{"cell_type":"markdown","source":["**Required Parameters**"],"metadata":{"id":"jwzaaEXxvPk7"}},{"cell_type":"code","source":["# clean data FILE to be errorified\n","input_file = \"/content/drive/MyDrive/artem-yushko/data-artem/cleaned/borshch4.txt\"\n","\n","# output FOLDER for the errorified and tagged data, future model input\n","out_folder = \"/content/drive/MyDrive/datasets/preprocessed/borshch4-punct-test\""],"metadata":{"id":"nqzzRfeVvNDj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Internals"],"metadata":{"id":"PNZq03E75ue6"}},{"cell_type":"code","source":["# all the imports we will need\n","import re\n","import os\n","import json\n","import time\n","import sys\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","\n","# local imports\n","sys.path.append('/content/drive/MyDrive/STABLE/helpers')\n","from SpaceHandler import SpaceHandler\n","\n","# initializing the imported classes\n","space_handler = SpaceHandler()"],"metadata":{"id":"q_cCYRLXx0A1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Matrix\n","Setting up the error probabilities"],"metadata":{"id":"nqYTxwpDe11v"}},{"cell_type":"code","source":["\"\"\" For simplicity, we create the transfer matrix between marks.\n","It incorporates deletions and additions by treating the absense of a mark\n","as _ -- also the punctuation mark, like the empty set.\n","transfer_matrix[',',';'] is the probability of comma to be converted to semicolon\n","\"\"\"\n","\n","marks = [' ', ',', ';', ':', chr(8212), '-', '.', '?', '!', chr(8230)] # Encoded punctuation marks\n","\n","transfer_matrix = pd.DataFrame(data = np.zeros((len(marks), len(marks))), \n","                index = marks, \n","                columns = marks)\n","\n","\"\"\"Hyperparameters of the errorifier. We concentrate on the most common mistakes\"\"\"\n","# Let us denote the relative odds of every error: p_1:p_2:p_3:...\n","transfer_matrix.loc[:, ' '] = 0.8 # deleting all punctuation in general\n","\n","transfer_matrix.loc[' ', ','] = 10 # extra comma\n","transfer_matrix.loc[' ', ';'] = 1 # extra semicolon\n","transfer_matrix.loc[' ', ':'] = 1 # extra colon\n","transfer_matrix.loc[' ', chr(8212)] = 2 # extra dash\n","transfer_matrix.loc[' ', '-'] = 1 # extra hyphen\n","\n","transfer_matrix.loc[',', ','] = 30 # kept comma\n","transfer_matrix.loc[',', ' '] = 80 # missed comma\n","transfer_matrix.loc[',', ';'] = 1 # comma by semicolon\n","\n","transfer_matrix.loc[';', ' '] = 5 # missed semicolon\n","transfer_matrix.loc[';', ','] = 20 # semicolon by comma\n","transfer_matrix.loc[';', chr(8212)] = 1 # semicolon by dash\n","\n","transfer_matrix.loc[':', ' '] = 1 # missed colon\n","transfer_matrix.loc[':', ','] = 3 # colon by comma\n","transfer_matrix.loc[':', chr(8212)] = 30 # colon by dash\n","\n","transfer_matrix.loc[chr(8212), ' '] = 90 # missed dash\n","transfer_matrix.loc[chr(8212), ','] = 30 # dash by comma\n","transfer_matrix.loc[chr(8212), ';'] = 1 # dash by semicolon\n","transfer_matrix.loc[chr(8212), ':'] = 5 # dash by colon\n","\n","transfer_matrix.loc['-', ' '] = 6 # missed hyphen\n","transfer_matrix.loc['-', chr(8212)] = 1 # hyphen by dash (uniting dash)\n","\n","transfer_matrix.loc[chr(8230), chr(8230)] = 1 # unchanged ellipsis\n","\n","transfer_matrix.loc['.', '?'] =  1 # into question\n","transfer_matrix.loc['.', '!'] =  1 # into assertion\n","transfer_matrix.loc['?', '.'] =  1 # no question\n","transfer_matrix.loc['?', '!'] =  1 # question into assertion\n","transfer_matrix.loc['!', '.'] =  1 # no assertion\n","transfer_matrix.loc['!', '?'] =  1 # assertion into question\n","\n","# How much of the dataset we want to have errors? A lot, I guess\n","\"\"\"\n","How many errors do we even want in one sentence? About 1 per 5 words\n","\"\"\"\n","probability_of_error = 5/10 # p of error on a spot\n","\n","for i in range(len(marks)-2):\n","  transfer_matrix.iloc[i,:] = probability_of_error*transfer_matrix.iloc[i,:]/np.sum(transfer_matrix.iloc[i,:])\n","  transfer_matrix.iloc[i,i] = 1 - probability_of_error\n","\n","# setting up custom probabilities for deleting the punctuation\n","to_spaces = .8\n","transfer_matrix.loc[:,' '] = to_spaces\n","norm = np.sum(transfer_matrix.iloc[:,1:], axis=1)\n","for m in marks[1:]:\n","  transfer_matrix.loc[:,m]=(1-to_spaces)*transfer_matrix.loc[:,m]/norm\n","\n","# Added by MB on Jun 25: increase prob of no change for spaces:spaces\n","no_change_if_space_prob = 0.95\n","transfer_matrix.loc[' ',' '] = no_change_if_space_prob\n","norm = np.sum(transfer_matrix.iloc[0:1,1:], axis=1)\n","multiplier = (1-no_change_if_space_prob)/float(norm)\n","for m in marks[1:]:\n","  transfer_matrix.loc[' ',m] = transfer_matrix.loc[' ',m]*multiplier\n","\n","def update_cell(char_from, char_to, prob):\n","  # set new prob\n","  transfer_matrix.loc[char_from, char_to] = prob \n","  # normalize row\n","  norm = np.sum(transfer_matrix.loc[char_from, transfer_matrix.columns != char_to])\n","  multiplier = (1-prob)/float(norm)\n","  transfer_matrix.loc[char_from, transfer_matrix.columns != char_to] = transfer_matrix.loc[char_from, transfer_matrix.columns != char_to]*multiplier\n","\n","update_cell(' ', ' ', 0.95) #to reduce number of deletes\n","update_cell(',', ' ', 0.4) #to reduce number of append_,\n","update_cell('.', ' ', 0.1) # to reduce number of append_.\n","update_cell('.', '?', 0.00005) #to reduce number of replace_.\n","update_cell('.', '!', 0.00005) #to reduce number of replace_.\n","update_cell('—', ' ', 0.90) #to increase number of append_-\n","update_cell(':', ' ', 0.90) #to increase number of append_:\n","update_cell('—', ',', 0.05) #to increase number of replace_-\n","update_cell(':', ',', 0.05) #to increase number of replace_:"],"metadata":{"id":"dNoqUuQ3e-Nh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Errorifier-Tagger\n","Combining the errorifying and tagging functions"],"metadata":{"id":"Eu2eCLja5DcL"}},{"cell_type":"code","source":["def tokenize_sentence(sentence):\n","  \"\"\"\n","  Creates a list of tokens, where each space between words is a separate token\n","  Example: [\"START\", \"\", \"Я\", \"\", \"вісім\", \"\", \"років\", \"\", \"бомбив\", \"\", \"Донбас\", \",\" \"вбив\", \"\", \"багатьох\", \":\", \"дітей\" \",\" \"дорослих\" \",\" \"і\", \"\", \"стариків\", \".\"]\n","  \"\"\"\n","  # preparing the data\n","  # getting rid of extra spaces\n","  sentence = space_handler.space_stripper(sentence)\n","  # replacing the ellipsis with one symbol\n","  sentence = re.sub(\"\\.\\.\\.\", chr(8230), sentence)\n","  # separating the quotation marks\n","  sentence = re.sub('\"', ' \" ', sentence)\n","  # clinging the punctuation back to those fuckers\n","  sentence = re.sub(r\"\\s([.,;:?!—-…])\", r\"\\1\", space_handler.space_stripper(sentence)) \n","  # fixing the contractions (ЭТО КОСТЫЛЬ, НУЖНО ЗАМЕНИТЬ В БЛИЖАЙШЕЕ ВРЕМЯ)\n","  sentence = re.sub(r'\\.,', chr(512), sentence)\n","  sentence = re.sub(r'\\.:', chr(513), sentence)\n","  sentence = re.sub(r'\\.;', chr(514), sentence)\n","  # matching the words\n","  words = re.findall(space_handler.uwr, sentence)\n","\n","  # matching the punctuation\n","  punctuation = re.split(space_handler.uwr, sentence)\n","\n","  # adding it as empty/non-empty tokens\n","  # fixing edge cases for our beloved quotation marks\n","  punctuation[0] = ' '\n","  if punctuation[-1] == '':\n","    punctuation[-1] = ' '\n","\n","  punctuation = [list(p)[0] for p in punctuation]\n","  tokens = ['$START']\n","  for i in range(len(words)):\n","      # adding everything to the token list\n","      tokens.append(punctuation[i])\n","      tokens.append(words[i])\n","  tokens.append(punctuation[-1])\n","  return tokens\n","\n","# generating the error\n","def generate_the_error(correct_mark):\n","  p = np.random.random()\n","  k = 0\n","  while p > transfer_matrix.iloc[marks.index(correct_mark),:k+1].sum(): # this is to choose the option with the given discrete distribution\n","    k += 1\n","  incorrect_mark = marks[k]\n","  return incorrect_mark"],"metadata":{"id":"kstpbMf66OLz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def errorify_and_tag(sentence, transfer_matrix=transfer_matrix):\n","  # tokenizing the sentence\n","  tokens = tokenize_sentence(sentence)\n","  # creating a list of labels of the same length\n","  labels = ['' for i in range(len(tokens))]\n","  \n","  # traversing through the list and generating errors for spaces between words\n","  for i in range(len(tokens)):\n","    # if it is a space, then the only option is to (de)generate a erroneous mark\n","    # so, the model needs to delete it\n","    if tokens[i] == ' ':\n","        imark = generate_the_error(' ')\n","        if imark != ' ': # if changed\n","          labels[i] = \"$DELETE\" # place the label\n","          tokens[i] = imark # put the incorrect mark in tokens instead of a space\n","        else:\n","          labels[i] = \"$KEEP\" # if nothing changed\n","\n","    # if it is a punctuation mark, then there are a few options which we might pick\n","    elif tokens[i] in marks:\n","        cmark = tokens[i] # retrieve the punctuation symbol\n","        imark = generate_the_error(cmark) # generate an error\n","\n","        if imark == ' ': # means that we have deleted cmark and need to put it back. thus we need to connect append to the previous word or the punctuation mark\n","            labels[i-1] = f\"$APPEND_{cmark}\"\n","        else: # we have not deleted it, so there are two options\n","          if cmark == imark: # nothing changed\n","            labels[i] = \"$KEEP\"\n","          else:  # changed into another non-empty symbol\n","            labels[i] = f\"$REPLACE_{cmark}\"\n","            \n","        # putting the incorrect mark in the token list\n","        tokens[i] = imark\n","    # else we just keep the element\n","    else:\n","        labels[i] = \"$KEEP\"\n","\n","    # making sure we haven't screwed anything up\n","    if len(tokens) != len(labels):\n","        print(\"Token list and label list do not match in length before space removal!\")\n","    assert(len(tokens) == len(labels))    \n","  return tokens, labels\n","\n","# removing the spaces (as in пробел) between words\n","def remove_space_tokens(tokens, labels):    \n","    for i in range(len(tokens)):\n","        # removing the tags from spaces\n","        if tokens[i] == ' ':\n","            labels[i] = ' '\n","    # removing both of those from the according lists\n","    tokens[:] = (t for t in tokens if t != ' ')\n","    labels[:] = (l for l in labels if l != ' ')\n","    # making sure that we haven't screwed anything up\n","    if len(tokens) != len(labels):\n","        print(\"Token list and label list do not match in length after space removal!\")\n","    assert(len(tokens) == len(labels))\n","    return tokens, labels\n","\n","# applying tags and converting back into the original sentence\n","def anti_tagger(tokens, labels):  \n","  # empty sentence to be filled\n","  sentence = ''\n","  # interpreting the tags\n","  if \"APPEND_\" in labels[0]: # if begins with append\n","    sentence += labels[0][-1]\n","  for i in range(1, len(tokens)):\n","    if labels[i] == '$KEEP': # if nothing changes\n","      sentence += tokens[i]\n","    elif labels[i] == '$DELETE': # if we delete the token\n","      sentence += '' # pass\n","    elif 'APPEND_' in labels[i]: # if we need to append one\n","      sentence += tokens[i] + labels[i][-1]\n","    elif 'REPLACE_' in labels[i]: # and if we need to replace\n","      sentence += labels[i][-1]\n","    else:\n","      print(\"Unidentified tag\")\n","    sentence += ' '\n","  # fixing the special symbols\n","  sentence = re.sub(chr(8230), '...', sentence)\n","  # returning\n","  sentence = re.sub(' \" ', '\"', sentence)\n","  sentence = re.sub(chr(512), '.,', sentence)\n","  sentence = re.sub(chr(513), '.:', sentence)\n","  sentence = re.sub(chr(514), '.;', sentence)\n","  return space_handler.fried_nails(sentence)\n","\n","# amalgaming all those functions together\n","def new_errorifier_tagger(sentence):\n","    # doing the actual work\n","    tokens, labels = errorify_and_tag(sentence)\n","    tokens, labels = remove_space_tokens(tokens, labels)\n","    return tokens, labels"],"metadata":{"id":"46DhFklX7F8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving the files"],"metadata":{"id":"NWfB8kThhHXI"}},{"cell_type":"code","source":["# reading the input data\n","with open(input_file, 'r') as f:\n","  text = f.read()\n","  lines = text.split('\\n')\n","\n","# creating the output folder\n","if not os.path.exists(out_folder):\n","  os.mkdir(out_folder)"],"metadata":{"id":"md8a2drP_eG-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(42) # for reproducibility\n","\n","final_list = []\n","t0 = time.time()\n","\n","print(\"Original length: \" + str(len(lines)) + \" sentences\")\n","\n","# traversing through the list\n","for i in range(len(lines)):\n","  l = lines[i]\n","  # making sure that the sentence is clean and ready to be preprocessed\n","  correct_sentence = space_handler.fried_nails(l)\n","  # making the error\n","  incorrect_sentence = new_errorifier_tagger(correct_sentence)\n","  # adding the sentence to the list\n","  # making sure that the interpreted sentence is the original one\n","  if anti_tagger(incorrect_sentence[0], incorrect_sentence[1]) == correct_sentence:\n","    final_list.append(incorrect_sentence)\n","  # else:\n","  #   print(l)\n","  # estimating the time left\n","  i += 1\n","  if not i % 10000:\n","    print(f\"{(time.time() - t0)/60:.1} mins elapsed so far. {i} sentences were processed\\nProjected time till the end: {(time.time() - t0)/3600/i*(len(lines)-i):.2} hours\")\n","\n","print(\"Errorified length: \" + str(len(final_list)) + \" sentences\")\n","\n","# splitting the dataset into train and dev\n","train, dev = train_test_split(final_list, test_size=0.2, random_state=47)\n","\n","# saving the train and the dev datasets\n","with open(out_folder + \"/train.json\", 'w') as f:\n","    json.dump(train, f) \n","\n","with open(out_folder + \"/dev.json\", 'w') as f:\n","    json.dump(dev, f) "],"metadata":{"id":"FDWrDqluhQcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dash_counter = 0\n","semicolon_counter = 0\n","\n","for i in range(len(final_list)):\n","  l = lines[i]\n","  correct_sentence = space_handler.fried_nails(l)\n","  if \"—\" in correct_sentence:\n","    dash_counter = dash_counter + 1\n","  if \",\" in correct_sentence:\n","    semicolon_counter = semicolon_counter + 1\n","\n","print(\"Total number of dashes: \" + str(dash_counter))\n","print(\"Total number of semicolons: \" + str(semicolon_counter))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F6KBEwyPgQmf","executionInfo":{"status":"ok","timestamp":1658255321838,"user_tz":240,"elapsed":97388,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"c1181cb9-782e-4618-8007-181d4ed7eef6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of dashes: 112758\n","Total number of semicolons: 693481\n"]}]},{"cell_type":"code","source":["# making the human-readable version of the data\n","\n","with open(out_folder + \"/human-readable.txt\", 'w') as f:\n","    # traversing through the list\n","    for sentence in final_list:\n","        # concatenating the list into a single errorified sentence\n","        new_sentence = space_handler.fried_nails(' '.join([sentence[0][i] + ' ' for i in range(len(sentence[0]))])[len(\"$START\"):])\n","\n","        # showing the tag-token alignment in a human-readable format\n","        tagged_sentence = ' '.join(sentence[0][i] + sentence[1][i] + ' ' for i in range(len(sentence[0])))\n","\n","        # Interpreting the sentence\n","        interpreted_sentence = anti_tagger(tagged_sentence[0], tagged_sentence[1])\n","\n","        # writing the sentences to the file\n","        f.write(new_sentence + '\\n' + tagged_sentence + '\\n' + interpreted_sentence + '\\n')"],"metadata":{"id":"uD1z1ZmKvqj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"LABEL COUNTER\"\n","all_labels = []\n","for sent in final_list:\n","  for label in sent[1]:\n","    all_labels.append(label)\n","\n","# count each label type\n","label_counts = dict(Counter(all_labels).items())\n","label_counts = dict(sorted(label_counts.items(), key=lambda item: item[1], reverse=True))"],"metadata":{"id":"OlKTQCsQgRny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving the metadata\n","message = \"########## Preprocess info ##########\\n\"\n","\n","# writing the datetime\n","ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","message += f\"Generation datetime: {ts}\\n\"\n","\n","# writing the sample\n","message += f\"Sample used: {input_file}\\n\"\n","\n","# writing the sentences size\n","message += f\"Number of sentences : {len(final_list)}\\n\"\n","\n","# writing the tokens size\n","message += f\"Number of tokens/tags : {len(all_labels)}\\n\"\n","\n","# writing the label vocab size\n","message += f\"Number of unique labels : {len(label_counts)}\\n\"\n","message += '\\n'\n","\n","# writing the label count\n","message += \"Label counts:\\n\"\n","for key in label_counts:\n","  message += f'{key} : {label_counts[key]}\\n'\n","\n","# saving the message itself\n","with open(out_folder + '/metadata.txt', 'w') as final_file:\n","  final_file.write(message)"],"metadata":{"id":"DNzwXJTBlLZK"},"execution_count":null,"outputs":[]}]}