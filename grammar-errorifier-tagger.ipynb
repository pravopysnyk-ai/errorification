{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"194-SXIF5OdWOFKxmks3k6vVSqXWU5Kw-","timestamp":1658765178540}],"collapsed_sections":["EztcRU5dH2AK","MA9tVoXQLhkz","UcH7uFPPifYZ","B153iuM-prlD"],"mount_file_id":"1etTs3PmPwP6MkS4q4sEuWth48_ceatHl","authorship_tag":"ABX9TyOohWTrn8ftUwARWXyuI5vl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["Grammar Errorifier-Tagger for the new HuggingFace training pipeline\n","\n","Written by Andre, Maksym, and Artem\n","\n","Last edit made by Artem: 07/08/2022  *(please edit this line if you change anything)*"],"metadata":{"id":"SizjfWmtxWur"}},{"cell_type":"code","source":["# clean data FILE to be errorified\n","input_file = \"/content/drive/MyDrive/artem-yushko/data-artem/cleaned/borshch4.txt\"\n","\n","# output FOLDER for the errorified and tagged data, future model input\n","out_folder = \"/content/drive/MyDrive/datasets/preprocessed/500k-grammar-borshch4\""],"metadata":{"id":"zYdQj_u6HXVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Internals"],"metadata":{"id":"EztcRU5dH2AK"}},{"cell_type":"markdown","source":["### Setup\n","\n","Setting up libraries, importing local files, initalizing ML models"],"metadata":{"id":"MA9tVoXQLhkz"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"MYk2xvpvpsaZ","executionInfo":{"status":"ok","timestamp":1675395094011,"user_tz":360,"elapsed":25807,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"outputs":[],"source":["# installing the necessary packages\n","!pip install spacy_udpipe &> /dev/null\n","!pip install pymorphy2 &> /dev/null\n","!pip install pymorphy2-dicts-uk &> /dev/null"]},{"cell_type":"code","source":["# all the imports we will need\n","import spacy_udpipe\n","import pymorphy2\n","import os\n","import time\n","import json\n","import random\n","import sys\n","import pandas as pd\n","from datetime import datetime\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","\n","# local imports\n","sys.path.append('/content/drive/MyDrive/STABLE/helpers')\n","from SpaceHandler import SpaceHandler\n","from Inflector import Inflector\n","from GrammarInterpreter import GrammarInterpreter"],"metadata":{"id":"dn6zFk6PIwwd","executionInfo":{"status":"ok","timestamp":1675395108357,"user_tz":360,"elapsed":14363,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e85572b4-1042-4d36-a4eb-c2cab419ca47"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]}]},{"cell_type":"code","execution_count":18,"metadata":{"id":"HZKWvjfUoSaa","executionInfo":{"status":"ok","timestamp":1675395319695,"user_tz":360,"elapsed":2489,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"outputs":[],"source":["# initializing the models\n","# spacy\n","spacy_path = '/content/drive/MyDrive/maksym-bondarenko/backend/models/ukrainian-iu-ud-2.5-191206.udpipe'\n","\n","SPACY_UDPIPE_MODEL = spacy_udpipe.load_from_path(\n","    lang=\"uk\",\n","    path=spacy_path,\n",")\n","\n","# pymorphy\n","morph = pymorphy2.MorphAnalyzer(lang='uk')"]},{"cell_type":"code","source":["# initializing the imported classes\n","space_handler = SpaceHandler()\n","inflector = Inflector(morph)\n","grammar_interpreter = GrammarInterpreter(space_handler, inflector)"],"metadata":{"id":"FKDr029NoDkn","executionInfo":{"status":"ok","timestamp":1675395323513,"user_tz":360,"elapsed":136,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["### Error Probabilities\n","\n","Probabilities for all kinds of errors"],"metadata":{"id":"UcH7uFPPifYZ"}},{"cell_type":"code","source":["# preposition error probability\n","p_mispreposition = 0.8\n","# misconjugation probability\n","p_misconjugation = 0.5"],"metadata":{"id":"XKUdYiZ-ik8x","executionInfo":{"status":"ok","timestamp":1675395108850,"user_tz":360,"elapsed":11,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Errorifier Functions\n","\n","Internal functions themselves"],"metadata":{"id":"B153iuM-prlD"}},{"cell_type":"code","source":["\"finds vidminok of a word given pos, word. uses inflector\"\n","def find_vidm(pos, word):\n","  descr = inflector.describe_word(pos, word)\n","  d = inflector.d_reverse[pos] # the dictionary of converting raw description to a tag\n","  seq = list()\n","  for i in d.keys():\n","   seq.append(len(set(i.split()) & descr)) \n","  return d[list(d.keys())[seq.index(max(seq))]] #шукає найбільше співпадіння між describe_word та словником описів відмінків\n","\n","\"splits text into (word, pos). uses spacy\"\n","def split_by_words(text):\n","  i = 0\n","  splitted_by_words = [('$START', 'PUNCT')] # we'll treat the starting token as punctuation to not trigger the mova-institute model\n","  for token in SPACY_UDPIPE_MODEL(text):\n","    splitted_by_words.append([\n","      token.text, #оригінал\n","      token.pos_, # частина мови\n","      ])\n","    i += 1\n","  return splitted_by_words\n","\n","# matches spacy pos to pymorphy pos\n","matchings = {\"PROPN\":\"NOUN\",\"NOUN\":\"NOUN\", \"VERB\":\"VERB\", \"PRON\":\"NPRO\", \"DET\":\"NPRO\",\"ADJ\":\"ADJF\", \"NUM\":\"NUMR\"} #match POS for inflector to be readable\n","\n","#list of prepositions fo preposition errors\n","preps = [\"від\", \"для\", \"по\", \"через\", \"при\", \"про\",\"згідно\", \"над\",\n","         \"під\", \"до\", \"з\", \"ради\", \"із\", \"зі\", \"на\", \"при\", \"за\", \"в\", \n","         \"на\", \"з-за\", \"із-за\", \"щодо\", \"крім\", \"між\", \"перед\", \"біля\"]\n","t_prep = tuple(preps)"],"metadata":{"id":"98tyL83cv2Mw","executionInfo":{"status":"ok","timestamp":1675395326089,"user_tz":360,"elapsed":123,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# helper functions\n","# moving appends to the previous tokens\n","def append_fix(tokens, labels, pos_seqs, conjugables, prepositions):\n","  labels_new = []\n","  for i in range(len(labels)):\n","    # if it is an append, then move it to the previous tag\n","    if labels[i].startswith('$APPEND'):\n","      labels_new[i-1] = labels[i]\n","    # and then append the existing tag\n","    labels_new.append(labels[i])\n","  return tokens, labels_new, pos_seqs, conjugables, prepositions\n","\n","# removing empty tokens\n","def remove_empty_tokens(tokens, labels, pos_seqs, conjugables, prepositions):\n","  tokens_new = [tokens[i] for i in range(len(tokens)) if tokens[i] != ''] # if a token is empty, it would not be added\n","  labels_new = [labels[i] for i in range(len(labels)) if tokens[i] != ''] # repeat everything for all other elements\n","  pos_seqs_new = [pos_seqs[i] for i in range(len(pos_seqs)) if tokens[i] != '']\n","  conjugables_new = [conjugables[i] for i in range(len(conjugables)) if tokens[i] != '']\n","  prepositions_new = [prepositions[i] for i in range(len(prepositions)) if tokens[i] != '']\n","  return tokens_new, labels_new, pos_seqs_new, conjugables_new, prepositions_new"],"metadata":{"id":"PcMpMpPfVmoT","executionInfo":{"status":"ok","timestamp":1675395326234,"user_tz":360,"elapsed":17,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# preparing the sentence for future errorifying\n","def prepare_sentence(sentence):\n","  tokens_and_pos = split_by_words(sentence) #use mova_institute to get tokens and part of speech for every word\n","  tokens =  [i[0] for i in tokens_and_pos]\n","  pos_seq = [i[1] for i in tokens_and_pos]\n","  conjugables = [pos in set(matchings) for pos in pos_seq]\n","  prepositions = [token.lower() in t_prep for token in tokens]\n","  labels = ['$KEEP' for i in range(len(tokens))]\n","  return tokens, labels, pos_seq, conjugables, prepositions\n","\n","# making an error in conjuctions\n","def make_conjugable_error(token, label, pos_seq, conjugable, preposition): # TAKES IN ONE WORD'S PROPERTIES\n","  pos = matchings[pos_seq] # convert to morph POS tags\n","  l = list(inflector.d_straight[pos].keys()) #список усіх відмінювань для частини мови даного слова\n","  vidm = l[random.randrange(1, len(l)-1)] #random case minus the default one and callings to fix the bug with plurals\n","  try: # if we can identify the original vidm\n","    new_label = \"$TRANSFORM_\" + pos + \"_\" + find_vidm(pos, token)\n","    new_token = inflector.inflect_word(token, vidm, pos)\n","    if token != new_token:\n","      return new_token, new_label, pos_seq, conjugable, preposition # return the errorified token + tag\n","    else: #if landed on the same one\n","      return token, label, pos_seq, conjugable, preposition # nothing changes\n","  except: # if can't identify vidm\n","    return token, label, pos_seq, conjugable, preposition # nothing changes"],"metadata":{"id":"VKmchrwvR0Q8","executionInfo":{"status":"ok","timestamp":1675395326236,"user_tz":360,"elapsed":17,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# make an error in prepositions\n","def make_preposition_error(token, label, pos_seq, conjugable, preposition): # TAKES IN ONE WORD'S PROPERTIES\n","  x = random.random()\n","  if x < 0.6: # make deletes a little more likely than replaces\n","    label = f'$APPEND_{token.lower()}'\n","    token = ''\n","  else: # if replace\n","    new = preps[random.randint(0, len(preps)-1)] #appennd random propostions\n","    if new in t_prep and new != token.lower(): #catches some weird bug\n","      label = f'$REPLACE_{token.lower()}'\n","      token = new\n","  return token, label, pos_seq, conjugable, preposition"],"metadata":{"id":"uBN37nenlIlR","executionInfo":{"status":"ok","timestamp":1675395326238,"user_tz":360,"elapsed":19,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# combine all the errorifying functions and apply them to a sentence\n","def errorify_sentence(sentence):\n","  # dissect the words by properties\n","  tokens, labels, pos_seq, conjugables, prepositions = prepare_sentence(sentence)\n","  # for each element (word/punct)\n","  for i in range(1, len(tokens)):\n","    # probability error\n","    p_error = random.random()\n","    # if it is a preposition and no tag has been applied to the previous token, make a preposition error\n","    if prepositions[i] and labels[i-1] == '$KEEP' and p_error <= p_mispreposition:\n","      tokens[i], labels[i], pos_seq[i], conjugables[i], prepositions[i] = make_preposition_error(tokens[i], labels[i], pos_seq[i], conjugables[i], prepositions[i])\n","    # if we can misconjunct the word and it does not have any appends, then make a conjugation error\n","    if conjugables[i] and labels[i] == '$KEEP' and p_error <= p_misconjugation:\n","      tokens[i], labels[i], pos_seq[i], conjugables[i], prepositions[i] = make_conjugable_error(tokens[i], labels[i], pos_seq[i], conjugables[i], prepositions[i])\n","  # append fix\n","  tokens, labels, pos_seq, conjugables, prepositions = append_fix(tokens, labels, pos_seq, conjugables, prepositions)\n","  # remove the empty tokens\n","  tokens, labels, pos_seq, conjugables, prepositions = remove_empty_tokens(tokens, labels, pos_seq, conjugables, prepositions)\n","  assert len(tokens) == len(labels)\n","  return tokens, labels"],"metadata":{"id":"AP8-fH-NhfGm","executionInfo":{"status":"ok","timestamp":1675395326240,"user_tz":360,"elapsed":20,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["errorify_sentence(\"Цьому ноутбуку двадцять років\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tm-NmC8oVLnz","executionInfo":{"status":"ok","timestamp":1675395334700,"user_tz":360,"elapsed":116,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"ad593c92-3d53-4ba5-9a51-8bcebb449c85"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['$START', 'Цьому', 'ноутбуку', 'двадцяти', 'років'],\n"," ['$KEEP', '$KEEP', '$KEEP', '$TRANSFORM_NUMR_N', '$KEEP'])"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## Saving the files"],"metadata":{"id":"feH8mBpq6yby"}},{"cell_type":"code","source":["# creating the output folder\n","if not os.path.exists(out_folder):\n","  os.mkdir(out_folder)\n","\n","# reading the file\n","with open(input_file, 'r') as f:\n","  text = f.read()\n","  lines = text.split('\\n')\n","  lines = lines[:500000]"],"metadata":{"id":"AhUYAhEJ6yby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_list = []\n","t0 = time.time()\n","misinterpreted_counter = 0\n","unprocessed_counter = 0\n","\n","# traversing through the list\n","for i in range(len(lines)):\n","  sentence = lines[i]\n","  # making the error\n","  # try except loop to catch the sentences not processed by pymorphy\n","  try:\n","    errorified = errorify_sentence(sentence)\n","    # adding the sentence to the list\n","    final_list.append(errorified)\n","  except:\n","      unprocessed_counter += 1\n","  # estimating the time left\n","  if i != 0 and not i % 10000:\n","      print(f\"{i} sentences were processed\\nProjected time till the end: {(time.time() - t0)/3600/i*(len(lines)-i):.2} hours\")\n","\n","# splitting the dataset into train and dev\n","train, dev = train_test_split(final_list, test_size=0.2, random_state=47)\n","\n","# showing the results\n","print(\"Out of 500, \" + str(unprocessed_counter) + \" sentences were not processed by Pymorphy and way too many could not be interpreted correctly.\")\n","\n","# saving the train and the dev datasets\n","with open(out_folder + \"/train.json\", 'w') as f:\n","    json.dump(train, f) \n","\n","with open(out_folder + \"/dev.json\", 'w') as f:\n","    json.dump(dev, f)"],"metadata":{"id":"RyzKTyAP6ybz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660009585237,"user_tz":240,"elapsed":11165638,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"009b4d00-9614-421c-cdeb-9a39389af4d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10000 sentences were processed\n","Projected time till the end: 3.1 hours\n","20000 sentences were processed\n","Projected time till the end: 3.0 hours\n","30000 sentences were processed\n","Projected time till the end: 2.9 hours\n","40000 sentences were processed\n","Projected time till the end: 2.9 hours\n","50000 sentences were processed\n","Projected time till the end: 2.8 hours\n","60000 sentences were processed\n","Projected time till the end: 2.7 hours\n","70000 sentences were processed\n","Projected time till the end: 2.7 hours\n","80000 sentences were processed\n","Projected time till the end: 2.6 hours\n","90000 sentences were processed\n","Projected time till the end: 2.5 hours\n","100000 sentences were processed\n","Projected time till the end: 2.5 hours\n","110000 sentences were processed\n","Projected time till the end: 2.4 hours\n","120000 sentences were processed\n","Projected time till the end: 2.4 hours\n","130000 sentences were processed\n","Projected time till the end: 2.3 hours\n","140000 sentences were processed\n","Projected time till the end: 2.2 hours\n","150000 sentences were processed\n","Projected time till the end: 2.2 hours\n","160000 sentences were processed\n","Projected time till the end: 2.1 hours\n","170000 sentences were processed\n","Projected time till the end: 2.0 hours\n","180000 sentences were processed\n","Projected time till the end: 2.0 hours\n","190000 sentences were processed\n","Projected time till the end: 1.9 hours\n","200000 sentences were processed\n","Projected time till the end: 1.9 hours\n","210000 sentences were processed\n","Projected time till the end: 1.8 hours\n","220000 sentences were processed\n","Projected time till the end: 1.7 hours\n","230000 sentences were processed\n","Projected time till the end: 1.7 hours\n","240000 sentences were processed\n","Projected time till the end: 1.6 hours\n","250000 sentences were processed\n","Projected time till the end: 1.5 hours\n","260000 sentences were processed\n","Projected time till the end: 1.5 hours\n","270000 sentences were processed\n","Projected time till the end: 1.4 hours\n","280000 sentences were processed\n","Projected time till the end: 1.4 hours\n","290000 sentences were processed\n","Projected time till the end: 1.3 hours\n","300000 sentences were processed\n","Projected time till the end: 1.2 hours\n","310000 sentences were processed\n","Projected time till the end: 1.2 hours\n","320000 sentences were processed\n","Projected time till the end: 1.1 hours\n","330000 sentences were processed\n","Projected time till the end: 1.1 hours\n","340000 sentences were processed\n","Projected time till the end: 0.99 hours\n","350000 sentences were processed\n","Projected time till the end: 0.93 hours\n","360000 sentences were processed\n","Projected time till the end: 0.87 hours\n","370000 sentences were processed\n","Projected time till the end: 0.8 hours\n","380000 sentences were processed\n","Projected time till the end: 0.74 hours\n","390000 sentences were processed\n","Projected time till the end: 0.68 hours\n","400000 sentences were processed\n","Projected time till the end: 0.62 hours\n","410000 sentences were processed\n","Projected time till the end: 0.56 hours\n","420000 sentences were processed\n","Projected time till the end: 0.5 hours\n","430000 sentences were processed\n","Projected time till the end: 0.43 hours\n","440000 sentences were processed\n","Projected time till the end: 0.37 hours\n","450000 sentences were processed\n","Projected time till the end: 0.31 hours\n","460000 sentences were processed\n","Projected time till the end: 0.25 hours\n","470000 sentences were processed\n","Projected time till the end: 0.19 hours\n","480000 sentences were processed\n","Projected time till the end: 0.12 hours\n","490000 sentences were processed\n","Projected time till the end: 0.062 hours\n","Out of 500, 0 sentences were not processed by Pymorphy and way too many could not be interpreted correctly.\n"]}]},{"cell_type":"code","source":["\"LABEL COUNTER\"\n","all_labels = []\n","for sent in final_list:\n","  for label in sent[1]:\n","    all_labels.append(label)\n","\n","# count each label type\n","label_counts = dict(Counter(all_labels).items())\n","label_counts = dict(sorted(label_counts.items(), key=lambda item: item[1], reverse=True))\n","\n","\n","\"METADATA WRITER\"\n","# saving the metadata\n","message = \"########## Preprocess info ##########\\n\"\n","\n","# writing the datetime\n","ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","message += f\"Generation datetime: {ts}\\n\"\n","\n","# writing the sample\n","message += f\"Sample used: {input_file}\\n\"\n","\n","# writing the sentences size\n","message += f\"Number of sentences : {len(final_list)}\\n\"\n","\n","# writing the tokens size\n","message += f\"Number of tokens/tags : {len(all_labels)}\\n\"\n","\n","# writing the label vocab size\n","message += f\"Number of unique labels : {len(label_counts)}\\n\"\n","message += '\\n'\n","\n","# writing the label count\n","message += \"Label counts:\\n\"\n","for key in label_counts:\n","  message += f'{key} : {label_counts[key]}\\n'\n","\n","# saving the message itself\n","with open(out_folder + '/metadata.txt', 'w') as final_file:\n","  final_file.write(message)"],"metadata":{"id":"ljDRL1JUNEja"},"execution_count":null,"outputs":[]}]}