{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ubtdFSrxG-MYLU9SV-8EmxHBqeQ25gSl","authorship_tag":"ABX9TyPOczBjaGkAZLN1A7h0ieeJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install spacy_udpipe &> /dev/null \n","!pip install pymorphy2 &> /dev/null \n","!pip install pymorphy2-dicts-uk &> /dev/null "],"metadata":{"id":"DGnGP_R8rKid","executionInfo":{"status":"aborted","timestamp":1678396856010,"user_tz":300,"elapsed":20,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Supporting classes\n"],"metadata":{"id":"4YJqugGnDa8B"}},{"cell_type":"code","source":["import re\n","\n","class SpaceHandler(object):\n","    \"\"\"\n","    handles spaces before and after punctuation\n","    functions:\n","    - space_stripper - strips extra spaces from text, used in space_oddity\n","    - space_oddity - adds extra spaces before punctuation for tokenization\n","    - fried_nails - removes extra spaces before punctuation for anti-tokenization\n","    \"\"\"\n","    def __init__(self):\n","        self.us = \"[А-ЩЬЮЯЄҐІЇЭЫЪа-щьюяєґіїэыъ'0-9a-zA-Z()%‰\\\"№\\+]\" # ukrainian word symbol + brackets + quotation marks + percentage sign (ыыы костыль) + russian symbols (ik but it is what it is)\n","        self.upr = r'[.?!,;:—-]' # ukrainian punctuation\n","        self.uwr = re.compile(self.us + \"+\") # Matches a word. We want our model to predict hyphens, thus I remove - from here\n","\n","    def space_stripper(self, sentence): # to get rid of extra spaces\n","        sentence = re.sub(r\"\\s{2,}\", ' ', sentence) # double+ spaces\n","        sentence = re.sub(r\"^\\s+\", '', sentence) # a space in the beginning (if double, then has already been removed)\n","        sentence = re.sub(r\"\\s+$\", '', sentence) # a space in the end\n","        sentence = re.sub(r'([0-9])([.?!,;:—-])\\s([0-9])', r\"\\1\\2\\3\", sentence) # spaces in punctuation between numbers\n","        return sentence\n","\n","    def space_oddity(self, sentence): # to add spaces in between of punctuation\n","        sentence = self.space_stripper(sentence) # get rid of extra spaces\n","        words = re.findall(self.uwr, sentence) # match words\n","        punctuation = re.split(self.uwr, sentence) # split the remains over words. The punctuation will be both at the beginning and in the end\n","        i = 0 # the index of considered punctuation\n","        sentence = \"\" # dummy for the newly created sentence\n","        while i < len(punctuation) - 1: # end before the last punctuation\n","            sentence += ' '.join(list(punctuation[i])) + ' ' +  words[i] + ' ' # the symbols between words now get to be joined by spaces. Likely with several spaces if there were spaces\n","            i += 1\n","        sentence += ' '.join(list(punctuation[-1])) # add the last punctuation to account for them not having the word following\n","        return self.space_stripper(sentence) # strip the remaining spaces just in case\n","\n","    def fried_nails(self, sentence): # the reversed function: to remove the extra spaces. Not 1-to-1 (or onto?), like the previous function\n","        sentence = re.sub('\\xad', '', sentence)\n","        words = re.findall(self.uwr, sentence) # retrieve the words as usual\n","        punctuation = re.split(self.uwr, sentence) # retrieve the rest\n","        i = 0\n","        sentence = \"\"\n","        while i < len(punctuation) -1:\n","            sentence += ''.join(re.split(r'\\s+', punctuation[i])) + ' ' +  words[i] # now we remove the convenient spaces from punctuation, losing info\n","            i += 1\n","        sentence += ''.join(re.split(r'\\s+', punctuation[-1]))\n","        sentence = re.sub(chr(8212), \" \" + chr(8212) + \" \", sentence) # the dash must be separated at all times, no matter what\n","        sentence = re.sub(r'\\s*-\\s*', \"-\", sentence) # the hyphen is considered to cling always\n","        quote_split = re.split(r'\\s*\"\\s*', sentence) # now, we deal with quotation marks\n","        sentence = \"\"\n","        for i in range(len(quote_split)//2):\n","            sentence += quote_split[2*i] + ' \"' + quote_split[2*i+1] + '\" ' # The odd numbered mark is the left one, the even numbered is the right one.\n","        if len(quote_split) % 2:\n","            sentence += quote_split[-1]\n","        # else: # if the number of marks is odd\n","            # print(\"Лапки порахуй, мудило\") # A suggestion to the user: \"Sorry, the program would work incorrectly if you do not fix the quotation marks yourself\"\n","        sentence = re.sub(r\"\\s([.,;:?!])\", r\"\\1\", self.space_stripper(sentence)) # The rest of the punctuation gets clinged\n","        sentence = re.sub(r\"\\(\\s+\", '(', sentence) # fix the left brackets avoiding the \"(\" case (three punctuation marks in a row)\n","        sentence = re.sub(re.compile(f\"({self.us})(\\()\"), r'\\1 \\2', sentence) # uncling the left bracket from a word\n","        sentence = re.sub(\"\\s+\\)\", ')', sentence) # in the same way\n","        sentence = re.sub('–', ' –', sentence) # put space before the dash\n","        sentence = re.sub(re.compile(f\"(\\))({self.us})\"), r'\\1 \\2', sentence) # uncling the right bracket from a word\n","        sentence = re.sub(r'\\s*-\\s*', \"-\", sentence) # the hyphen is considered to cling always\n","        sentence = re.sub(r\"\\s*’\\s*\", \"’\", sentence) # same for apostrophes\n","        sentence = re.sub(r\"’\\s*\", \"’\", sentence) # same for apostrophes\n","        sentence = re.sub(r'\"\\s\\(', '\"(', sentence) # removing the space from \" (\n","        sentence = re.sub(r'\\)\\s\"', ')\"', sentence) # removing the space from ) \"\n","        sentence = re.sub(r'([0-9])([.?!,;:—-])\\s([0-9])', r\"\\1\\2\\3\", sentence) # spaces in punctuation between numbers\n","        return sentence"],"metadata":{"id":"77KJjPbAtaZS","executionInfo":{"status":"aborted","timestamp":1678396856010,"user_tz":300,"elapsed":1028,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","# class used to extract data about word (in context, such as part of language, gender, etc), and inflect the word\n","class FormExtractorInflector():\n","    matchings_part = {\"VERB\":\"VERB\", \"PRON\":\"NPRO\", \"DET\":\"NPRO\",\"ADJ\":\"ADJF\", \"NUM\":\"NUMR\", \"NOUN\":\"NOUN\"} #match POS for inflector to be readable\n","\n","    #cases from movainstitute to inflector\n","    cases = {\"Nom\":\"N\", \"Gen\": \"R\", \"Dat\":\"D\", \"Acc\":\"Z\", \"Ins\":\"O\", \"Loc\":\"M\", \"Voc\":\"K\"}\n","\n","    v_forms = {\"Inf\" : [\"Inf\"], \"Pr1\": [\"Pres\", \"1\"], \"Pr2\" : [\"Pres\", \"2\"],\n","                    \"Pr3\" : [\"Pres\", \"3\"], \"Fu1\" : [\"Fut\", \"1\"], \"Fu2\" : [\"Fut\", \"2\"],\n","                    \"Fu3\" : [\"Fut\", \"3\"], \"PsMs\" : [\"Past\", \"Masc\"], \"PsFe\" : [\"Past\", \"Fem\"],\n","                    \"PsNe\" : [\"Past\", \"Neut\"]}\n","    genders = {\"Masc\":\"masc\", \"Fem\":\"femn\", \"Neut\":\"neut\"}\n","    quanitys = {\"Plur\":\"plur\", \"Sing\":\"sing\"}\n","    aspects = {\"Perf\":\"perf\", \"Imp\":None}\n","\n","    v_forms_rev = {\" \".join(v): k for k, v in v_forms.items()}\n","    def __init__(self, inflector):\n","        self.infl=inflector\n","\n","    # helper function, just to save code\n","    def c(self, a):\n","        if a is None: return None\n","        if a in self.genders: return self.genders[a]\n","        if a in self.quanitys: return self.quanitys[a]\n","        if a in self.cases: return self.cases[a]\n","        if a in self.v_forms_rev: return self.v_forms_rev[a]\n","        if a in self.aspects: return self.aspects[a]\n","        return a\n","\n","    # get data about word using Mova Institute\n","    # sentence_anal is a sentence after Mova Institute (array of words with info)\n","    # this function extracts main info about word at index - index\n","    def extract_data(self, sentence_anal, index):\n","        # not really needed, but changes from [[00,01,02], [10,11,12]] to [[00, 10], [01, 11], [02, 12]]\n","        anal = list(zip(*sentence_anal))\n","\n","        # word itself\n","        source = anal[0][index]\n","        # which part of language it is\n","        part = anal[2][index]\n","        # matching_part is a dictionary just to change part naming from Mova Institute to inflector\n","        if part in self.matchings_part:\n","            part = self.matchings_part[part]\n","        #descr contains all other info (like gender, plural, etc.)\n","        descr = anal[4][index]\n","        descr = descr.split(\"|\")\n","        \n","        case = None\n","        gender = None\n","        quantity = None\n","        verbform = None\n","        tense = None\n","        person = None\n","        aspect = None\n","        mood = None\n","\n","        for i in descr:\n","            ir = i.split(\"=\")\n","            if ir[0]==\"Case\":\n","                case = ir[1]\n","            elif ir[0]==\"Gender\":\n","                gender = ir[1]\n","            elif ir[0]==\"Number\":\n","                quantity = ir[1]\n","            elif ir[0]==\"VerbForm\":\n","                verbform = ir[1]\n","            elif ir[0]==\"Tense\":\n","                tense = ir[1]\n","            elif ir[0]==\"Person\":\n","                person = ir[1]\n","            elif ir[0]==\"Aspect\":\n","                aspect = ir[1]\n","            elif ir[0]==\"Mood\":\n","                mood = ir[1]\n","        \n","        #return info about word\n","        # case/Inf/etc, part of language, gender, quantity, aspect, word itself\n","        if case is not None:\n","            return self.c(case), part, self.c(gender), self.c(quantity), self.c(aspect), source\n","        if verbform == \"Inf\":\n","            return \"Inf\", part, self.c(gender), self.c(quantity), self.c(aspect), source\n","        if tense is not None:\n","            if person is None and gender is not None:\n","                return self.c(\" \".join([tense, gender])), part, None, None, None, source#c(aspect)\n","            else:\n","                if person is None: person =\"1\"\n","                return self.c(\" \".join([tense, person])), part, self.c(gender), self.c(quantity), self.c(aspect), source\n","        if mood==\"Imp\" and verbform==\"Fin\":\n","            if person is None: person=\"1\"\n","            return \"Imp\"+person,part,self.c(gender),self.c(quantity),self.c(aspect)\n","        return None, part, None, None, None, source\n","    \n","    # inflects word if it is possible. If not (bcs it is verb, etc.) then returns the original word\n","    def inflect_word_ignore_part(self, word, case, part, gender=None, quantity=None, perf=None):\n","        try:\n","            return self.infl.inflect_word(word, case, part, gender=gender, quantity=quantity), True\n","        except Exception as e:\n","            print(e)\n","            for i in self.matchings_part.keys():\n","                try:\n","                    return self.infl.inflect_word(word, case, self.matchings_part[i], gender=gender, quantity=quantity), True\n","                except Exception as e:\n","                    print(e)\n","                    pass\n","        return word, False\n","    # checks if ending of words is the same (to avoid ся vs сь, etc.)\n","    def word_inflection_goodness_checker(self, word1, word2):\n","        score=0\n","        if word1[-1]==word2[-1]:\n","            score+=100\n","        if word1[-1]==word2[-1] and word1[-2]==word2[-2]:\n","            score+=50\n","        return score\n","\n","    # gets data about word at index index in sentence, then puts \"word\" in the same form (case, gender, etc.) \n","    def inflect_as_in_sentence(self, sentence_anal, index, word):\n","        # extract data\n","        case, part, gender, quantity, aspect, source = self.extract_data(sentence_anal, index)\n","        # return original if X part\n","        if part == \"ADV\" or part==\"PUNCT\" or part == \"ADP\":\n","            return word, True\n","        try:\n","            # case 1, \"case\" of the word is Z\n","            if case==\"Z\":\n","                # to fix some situations with inflector, it tries Z and N \"cases\", then selects best one, based on ending similarity\n","                inflection1 = self.inflect_word_ignore_part(word, case, part, gender=gender, quantity=quantity, perf=aspect)\n","                score1 = self.word_inflection_goodness_checker(inflection1[0], source)\n","                inflection2 = self.inflect_word_ignore_part(word, \"N\", part, gender=gender, quantity=quantity, perf=aspect)\n","                score2 = self.word_inflection_goodness_checker(inflection2[0], source)\n","                if score1>=score2:\n","                    return inflection1\n","                else:\n","                    return inflection2\n","            # case 2, \"part\" of the word is verb\n","            if part==\"VERB\":\n","                inflection = self.inflect_word_ignore_part(word, case, part, gender=gender, quantity=quantity, perf=aspect)\n","                \n","                # check for ending, ся vs сь\n","                score1 = self.word_inflection_goodness_checker(inflection[0][:-1]+\"я\", source)\n","                score2 = self.word_inflection_goodness_checker(inflection[0], source)\n","                if score1>=score2:\n","                    inflection=(inflection[0][:-1]+\"я\", inflection[1])\n","                return inflection\n","            # simplest case, covers most situations\n","            return self.inflect_word_ignore_part(word, case, part, gender=gender, quantity=quantity, perf=aspect)\n","        except Exception as e:\n","            print(e)\n","            #print(\"return original word in inflect_as_in_sentence\")\n","            return word, False\n","        pass"],"metadata":{"id":"vh9CWtdGLeGc","executionInfo":{"status":"aborted","timestamp":1678396856347,"user_tz":300,"elapsed":2,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SurzhiksGenerator(FormExtractorInflector):\n","\n","        \n","    def __init__(self, inflector, spacy_udpipe_model, pairs_path='/content/drive/MyDrive/ukramarly-main/errorifier/errorifier-data/lexical/surzhiks.txt'):\n","        self.SPACY_UDPIPE_MODEL=spacy_udpipe_model\n","        super().__init__(inflector=inflector)\n","        def pairs_analyzer(pairs):\n","            res=[]\n","            for i in range(len(pairs)):\n","                res.append([self.analyze_sentence(pairs[i][0]), self.analyze_sentence(pairs[i][1])])\n","            return res\n","        def bye_parentheses(mystring):\n","            start = mystring.find(\"(\")\n","            end = mystring.find(\")\")\n","            if start != -1 and end != -1:\n","                return mystring[start+1:end]\n","            else:\n","                return mystring\n","        def clear_spaces(sentence):\n","            if sentence[0]==\" \":\n","                sentence = sentence[1:]\n","            if sentence[-1]==\" \":\n","                sentence = sentence[:-1]\n","            return sentence\n","\n","\n","        \"\"\"\n","        формат суржиків:\n","        в списку:\n","            не правильно - правильно\n","        в коді:\n","            suzhiks = [\n","                [правильно, не правильно],\n","                [правильно, не правильно],\n","                ...\n","            ]\n","\n","        \"\"\"\n","        common_errors_path = pairs_path\n","        surzhiks2 = []\n","        with open(common_errors_path, encoding = 'utf-8', mode = 'r') as file:\n","            for line in file:\n","                    if len(line)==0:\n","                      continue\n","                    if line[0]==\"#\":\n","                      continue\n","                    if line[:2]==\"\\n\":\n","                      continue\n","                    words = line.split(\"; \")\n","                    # MODIFIED THE FOLLOWING TWO LINES\n","                    left = clear_spaces(bye_parentheses(words[1]))\n","                    right = clear_spaces(bye_parentheses(words[0].replace(\"\\n\",\"\")))\n","                    left_arr=left.split(\" \")\n","                    rights=right.split(\",\")\n","                    right_arr=[]\n","                    for i in rights:\n","                        right_arr=i.split(\" \")\n","                        if len(left_arr)!=len(right_arr):\n","                            right_arr=[]\n","                    if len(right_arr)==0 or len(right_arr)==1: continue\n","\n","                    # for i in range(len(right_arr)):\n","                    #     surzhiks2.append([right_arr[i], left_arr[i]])\n","\n","                    surzhiks2.append([\" \".join(right_arr),left])\n","                    #words[0] - правильна фраза, words[1] - неправильна фраза\n","\n","            file.close()\n","        self.surzhiks2 = surzhiks2\n","        self.pairs_anal=pairs_analyzer(surzhiks2)\n","        pass\n","\n","    def analyze_sentence(self, text: str):\n","        i = 0\n","        splitted_by_words = []\n","        for token in self.SPACY_UDPIPE_MODEL(text):\n","            splitted_by_words.append ([\n","                token.text, #оригінал\n","                token.lemma_, # лема\n","                token.pos_, # частина мови\n","                token.dep_, # синтаксична роль\n","                str(token.morph),# морфологічний опис слова \n","                0 if token.i == (token.head.i) else token.head.i #індекс голови\n","            ])\n","            i += 1\n","        return splitted_by_words\n","\n","    def smart_n_grams_creator(self,anal):\n","        smart_n_grams = []\n","        for i in range(len(anal)):\n","            anal[i].append(i)\n","        for n in range(min(len(anal),4)):\n","            for i in range(len(anal)):\n","                n_gram = {}\n","                n_gram[i] = anal[i][5]\n","                last = i\n","                for j in range(len(anal)):\n","                    if j == n-1:\n","                        break\n","                    ind = n_gram[last]\n","                    n_gram[ind] = anal[ind][5]\n","                    last = ind\n","                    if n_gram[ind] == ind:\n","                        break\n","                n_gram_words = []\n","                for j in n_gram.keys():\n","                    n_gram_words.append(anal[j])\n","                smart_n_grams.append(n_gram_words)\n","        return smart_n_grams\n","    def arePermutations(self, arr1, arr2):\n","        if (len(arr1) != len(arr2)):\n","            return False\n","        hM = defaultdict (int)\n","        for i in range (len(arr1)):\n","            x = arr1[i]\n","            hM[x] += 1\n","        for i in range (len(arr2)):\n","            x = arr2[i]\n","            if x not in hM or hM[x] == 0:\n","                return False\n","            hM[x] -= 1\n","        return True\n","    def smart_n_grams_clearer(self,smart_n_grams_array):\n","        output = []\n","        for i in range(len(smart_n_grams_array)):\n","            for j in range(len(smart_n_grams_array)):\n","                if i==j:\n","                    continue\n","                anal1 = list(zip(*smart_n_grams_array[i]))\n","                anal2 = list(zip(*smart_n_grams_array[j]))\n","                if self.arePermutations(anal1[0], anal2[0]):\n","                    smart_n_grams_array[i] = \"TAGGG\"\n","        \n","        for i in smart_n_grams_array:\n","            if i!=\"TAGGG\":\n","                output.append(i)\n","        return output\n","    def equality_checker(self, pair1_anal, pair2_anal, indexes=None):\n","        anal1 = list(zip(*pair1_anal))\n","        norm1=anal1[1]\n","        anal2 = list(zip(*pair2_anal))\n","        norm2=anal2[1]\n","        index_relation = {}\n","        fl = 0\n","        tf1=0\n","        tf2=0\n","        for i in range(len(norm1)):\n","            if anal1[2][i]!=\"PUNCT\":\n","                tf1+=1\n","        for i in range(len(norm2)):\n","            if anal2[2][i]!=\"PUNCT\":\n","                tf2+=1\n","        if tf1!=tf2:\n","            return False, index_relation\n","        for i in range(len(norm1)):\n","            for j in range(len(norm2)):\n","                if norm1[i]==norm2[j]:\n","                    if indexes is not None:\n","                        index_relation[j] = indexes[i]\n","                    else:\n","                        index_relation[j] = i\n","                    fl+=1\n","                    break\n","                elif anal1[2][i]==\"PUNCT\" or anal2[2][j]==\"PUNCT\":\n","                    continue\n","        return fl==tf1, index_relation\n","    def prepare_subsitution(self,sentence_anal, pair_anals, indexes, keep_if_same=True):\n","        anal = list(zip(*sentence_anal))\n","        pair0=list(zip(*(pair_anals[0])))\n","        pair=list(zip(*(pair_anals[1])))\n","        pair_inflected = []\n","        inflected_successfuly = True\n","        for i in range(min(len(pair[0]), len(pair0[0]))):\n","            success = False\n","            if pair0[0][i] == anal[0][indexes[i]] and keep_if_same:\n","                infl_pair, success = pair[0][i], True\n","            else:\n","                infl_pair, success = self.inflect_as_in_sentence(sentence_anal, indexes[i], pair[0][i])\n","            if not success:\n","                inflected_successfuly = False\n","            pair_inflected.append(infl_pair)\n","        return pair_inflected, inflected_successfuly\n","    def get_indexes_for_substitution(self,n_grams, pair_anal):\n","        for i in range(len(n_grams)):\n","            indexes = {ind : n_grams[i][ind][6] for ind in range(len(n_grams[i]))}\n","            equality, connection = self.equality_checker(n_grams[i], pair_anal[0], indexes=indexes)\n","            if equality:\n","                return True, connection, n_grams[i]\n","        return False, None, None\n","    def substitude(self,anal, smart_n_grams, pair_anal,keep_if_same=True):\n","        sentence_arr=list(list(zip(*anal))[0])\n","        found, indexes, _ = self.get_indexes_for_substitution(smart_n_grams, pair_anal)\n","        changed = True\n","        if not found:\n","            tokens = [\"$KEEP\" for i in range(len(sentence_arr))]\n","        else:\n","            inflected_pair, inflected_successfuly = self.prepare_subsitution(anal, pair_anal, indexes, keep_if_same=keep_if_same)\n","            changed = inflected_successfuly\n","            tokens = [\"$KEEP\" for i in range(len(sentence_arr))]\n","            for i in indexes.keys():\n","                sentence_arr[indexes[i]] = inflected_pair[i]\n","                tokens[indexes[i]] = \"$ANTISURZHIFY\"\n","        return sentence_arr, tokens, changed\n","    def sentence_adj_changer2(self, sentence):\n","        anal = self.analyze_sentence(sentence)\n","        ress = list(zip(*anal))\n","        result = list(ress[0])\n","        for i in range(len(ress[0])):\n","            if ress[2][i]==\"NOUN\":\n","                for j in range(len(ress[0])):\n","                    if ress[2][j]==\"ADJ\" and ress[5][j]==i:\n","                        result[j]=self.inflate_word_as_regardless(ress[0][i],ress[0][j])\n","        return \" \".join(result)\n","    def smart_n_grams_token_clearer(self,n_grams):\n","        res = []\n","        for i in n_grams:\n","            fl=True\n","            for desc in i:\n","                if desc[3]!=\"$ANTISURZHIFY\":\n","                    fl=False\n","            if fl:\n","                res.append(i)\n","        return res\n","    def antisurzhifier(self, sentence, tokens=None):\n","        anal = self.analyze_sentence(sentence)\n","        if tokens is not None:\n","          for i in range(len(anal)):\n","              anal[i][3]=tokens[i]\n","        if tokens is None:\n","          smart_n_grams = (self.smart_n_grams_clearer(self.smart_n_grams_creator(anal)))\n","        else:\n","          smart_n_grams = self.smart_n_grams_token_clearer(self.smart_n_grams_clearer(self.smart_n_grams_creator(anal)))\n","        pair_anal = self.pairs_anal[0]\n","        res=self.substitude(anal, smart_n_grams, pair_anal[::-1], keep_if_same=True)\n","        sentence_res = res[0]\n","        for pair_anal in self.pairs_anal:\n","            if sentence_res!=list(list(zip(*anal))[0]):\n","                anal = self.analyze_sentence(\" \".join(sentence_res))\n","            if tokens is None:\n","              smart_n_grams = (self.smart_n_grams_clearer(self.smart_n_grams_creator(anal)))\n","            else:\n","              for i in range(len(anal)):\n","                  anal[i][3]=tokens[i]\n","              smart_n_grams = self.smart_n_grams_token_clearer(self.smart_n_grams_clearer(self.smart_n_grams_creator(anal)))\n","            res=self.substitude(anal, smart_n_grams, pair_anal[::-1], keep_if_same=True)\n","            sentence_res=res[0]\n","        result=\"\"\n","        for i in sentence_res:\n","            puncts=[',','.',':',\";\",'-']\n","            if i in puncts:\n","                result+=i\n","            else:\n","                result+=\" \"+i\n","        return result[1:]"],"metadata":{"id":"bgNGACvwrV_d","executionInfo":{"status":"aborted","timestamp":1678396856348,"user_tz":300,"elapsed":3,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy_udpipe\n","import pymorphy2\n","\n","import sys\n","sys.path.append(\"/content/drive/MyDrive/STABLE/helpers\")\n","\n","# import inflector\n","from Inflector import Inflector\n","\n","morph = pymorphy2.MorphAnalyzer(lang='uk')\n","inflector = Inflector(morph)\n","\n","\n","model_path = '/content/drive/MyDrive/model-directories/mova_institute.udpipe'\n","SPACY_UDPIPE_MODEL = spacy_udpipe.load_from_path(\n","lang=\"uk\",\n","path=model_path\n",")"],"metadata":{"id":"8MrKdpnueWDg","executionInfo":{"status":"aborted","timestamp":1678396856348,"user_tz":300,"elapsed":2,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Errorify dataset"],"metadata":{"id":"R4PmvZgmqQ9O"}},{"cell_type":"code","source":["# init\n","surzhik_generator = SurzhiksGenerator(inflector, SPACY_UDPIPE_MODEL)"],"metadata":{"id":"oIjVQseQkTto","executionInfo":{"status":"ok","timestamp":1678395684456,"user_tz":300,"elapsed":1292,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["surzhik_generator.antisurzhifier(\"я вимкнув світло\")"],"metadata":{"id":"ebIh3i-CPXEf","executionInfo":{"status":"aborted","timestamp":1678396856348,"user_tz":300,"elapsed":2,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# clean data FILE to be errorified\n","input_file = \"/content/drive/MyDrive/artem-yushko/data-artem/cleaned/borshch4.txt\"\n","\n","# output FOLDER for the errorified and tagged data, future model input\n","out_folder = \"/content/drive/MyDrive/UNLP/assist-data/surzhik-generated-1k\""],"metadata":{"id":"PnbYSUOZqsJw","executionInfo":{"status":"ok","timestamp":1678395684457,"user_tz":300,"elapsed":5,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import os\n","# creating the output folder\n","if not os.path.exists(out_folder):\n","  os.mkdir(out_folder)\n","\n","# reading the file\n","with open(input_file, 'r') as f:\n","  text = f.read()\n","  lines = text.split('\\n')\n","\n","dataset = lines"],"metadata":{"id":"_uQf8RLGqtrM","executionInfo":{"status":"ok","timestamp":1678395691979,"user_tz":300,"elapsed":7526,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["space_handler = SpaceHandler()"],"metadata":{"id":"iu0LU1RgtvFO","executionInfo":{"status":"ok","timestamp":1678395691980,"user_tz":300,"elapsed":5,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# extract ukr key phrases which to look for\n","ukr_key_phrases = [word[1][:-1] for word in surzhik_generator.surzhiks2]\n","# convert it to pairs of keywords\n","ukr_key_words = []\n","for key_phrase in ukr_key_phrases:\n","  words = key_phrase.split(\" \")\n","  ukr_key_words.append(words)\n","\n","# extract ids of relevant sentences (those that contain all key words for given keyword pair)\n","relevant_id = {}\n","for key_phrase in ukr_key_phrases:\n","  relevant_id[key_phrase] = []\n","for sent_id in range(len(dataset)):\n","  sentence = space_handler.space_oddity(dataset[sent_id])\n","  for key_phrase_id in range(len(ukr_key_phrases)):\n","    phrase_present = True\n","    for word in ukr_key_words[key_phrase_id]:\n","      # if any word is not present, phrase_present = false\n","      el = \" \" + word + \" \"\n","      phrase_present = phrase_present and (el in sentence)\n","    if phrase_present:\n","      relevant_id[ukr_key_phrases[key_phrase_id]].append(sent_id)\n","\n","# extract # of  sentences for each category \n","for key_phrase in ukr_key_phrases:\n","  print(key_phrase + ': ' + str(len(relevant_id[key_phrase])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"2X63T5guSpLf","executionInfo":{"status":"error","timestamp":1678396856009,"user_tz":300,"elapsed":25,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"8c48f5d9-ec64-491f-c403-98ae54cc896f"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a6a28275da00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extract ukr key phrases which to look for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mukr_key_phrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msurzhik_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurzhiks2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# convert it to pairs of keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mukr_key_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey_phrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mukr_key_phrases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'surzhik_generator' is not defined"]}]},{"cell_type":"code","source":["# extract 20 sentences for each category\n","idxs_to_surzhify = []\n","for key_phrase in ukr_key_phrases:\n","  idxs_to_surzhify += relevant_id[key_phrase][:20]\n","\n","sentences_to_surzhify = [dataset[id] for id in idxs_to_surzhify]\n","len(sentences_to_surzhify)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRSMpS9tYGFw","executionInfo":{"status":"ok","timestamp":1678395887502,"user_tz":300,"elapsed":6,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"f8bb8ae9-1284-468d-dabd-496fae7bbfc1"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1098"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# now only errorify relecant sengences\n","import time\n","s = time.time()\n","out_sentences =[]\n","for sent in sentences_to_surzhify:\n","  try:\n","    out_sentence = surzhik_generator.antisurzhifier(sent)\n","    out_sentence = space_handler.fried_nails(out_sentence)\n","  except:\n","    out_sentence = sent\n","  out_sentences.append(out_sentence)\n","print(time.time() - s)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPYj7NAFYwzm","outputId":"4cfc6a55-b5c7-4a17-f412-64c01db45894"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["list index out of range\n","list index out of range\n","list index out of range\n","list index out of range\n","list index out of range\n","list index out of range\n","list index out of range\n","list index out of range\n","list index out of range\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hH17i0tjYK6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = '\\n'.join(out_sentences)\n","with open(out_folder + \"/source.txt\", 'w') as f:\n","  f.writelines(text)\n","\n","text = '\\n'.join(sentences_to_surzhify)\n","with open(out_folder + \"/target.txt\", 'w') as f:\n","  f.write(text)"],"metadata":{"id":"hAIC-xbpopkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QMcgActfYQdU"},"execution_count":null,"outputs":[]}]}